# ğŸ§  LLM Chat Web App with Embeddings & ChromaDB

This project is a microservices-based web application that allows you to:

- ğŸ•¸ï¸ Scrape and parse website content

- ğŸ“ Convert it into Markdown

- ğŸ§¬ Generate vector embeddings using OpenAI

- ğŸ§  Store them in ChromaDB

- ğŸ’¬ Query them using an LLM (OpenAI or Gemini)

- ğŸ–¥ï¸ Interact via a simple frontend

---

## ğŸ“¦ Services Overview

| Service              | Description                                                                 |
|----------------------|-----------------------------------------------------------------------------|
| `scraper-embeddings` | Scrapes website content into markdown, generates embeddings with OpenAI     |
| `chroma-db`          | Vector store using ChromaDB with persistent volume                          |
| `llm`                | Backend API that processes queries and enhances responses using Gemini      |
| `frontend`           | Web interface to interact with the chat system                              |

---

## ğŸ“Š Architecture Diagram

![Diagram](assets/llm-chat.png)

---

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/your-user/llm-chat-app.git
cd llm-chat-app
```

### 2. Setup Environment Variables
Create a .env file at the root of the project with the following content:

```bash
OPENAI_API_KEY=your_openai_key
GEMINI_API_KEY=your_gemini_key
ANONYMIZED_TELEMETRY=FALSE
```
    âš ï¸ Make sure these variables are accessible by Docker Compose when running the containers.

Alternatively, you can export them manually:

```bash
export OPENAI_API_KEY=your_openai_key
export GEMINI_API_KEY=your_gemini_key
```

### 3. Run the App
Make sure Docker is running, then launch everything:

```bash
docker compose up --build
```

This will spin up:
- ChromaDB on `http://localhost:8000`
- Scraper/Embedding API on `http://localhost:8080`
- LLM API on `http://localhost:5050`
- Frontend on `http://localhost:3000`

---

## ğŸ“š Adding Dictionary Data
You can enrich LLM responses with additional structured context using a dictionary file:

- Place your custom dictionary.json inside llm/data/

- This file should contain extra definitions, acronyms, or reference data

- The LLM backend will use this during the query process to improve accuracy and context

---

## ğŸ“‚ Folder Structure

```bash
.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ llm-chat.png
â”œâ”€â”€ compose.yml
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ createEmbeddingsDB.py
â”‚       â”œâ”€â”€ extractWebInfo.py
â”‚       â””â”€â”€ scraper.py
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ app.js
â”‚   â”œâ”€â”€ index.html
â”‚   â””â”€â”€ style.css
â””â”€â”€ llm/
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ app.py
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ dictionary.json
    â”‚   â””â”€â”€ config.json
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ utils/
        â””â”€â”€ llm.py
```

---

## ğŸ› ï¸ Tips & Troubleshooting
- âœ… Make sure .env is not accidentally ignored or overwritten

- ğŸ”„ Restart containers if you change .env or environment variables

- ğŸ“ Embeddings are saved inside the ChromaDB persistent volume under ./chromadb/

---

## ğŸ“˜ License
GNU GENERAL PUBLIC License â€” see LICENSE for details.